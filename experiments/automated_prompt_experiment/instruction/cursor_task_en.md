# Cursor Task: Automated Prompt Experimentation with Higgs Audio v2

## üéØ Objective
Extend the current Higgs Audio v2 pipeline to support **automated experimentation** over prompts, system messages, scene descriptions, and decoding parameters ‚Äî without altering the model‚Äôs core logic.

Goals:
1. Generate a structured set of **text prompts** (content sentences).
2. Synthesize audio for each prompt via the Higgs API (system + `[SPEAKER*]` tags + references).
3. Evaluate each output for **speaker embedding similarity**, **WER**, and **latency**.
4. Store all results in a single **CSV** for analysis.

---

## üß± Step 1 ‚Äì Prompt Generation
- Use `generate_prompt_texts.py` to generate **50‚Äì100** text sentences across 10 styles: neutral, conversational, formal, announcement, calm, excited, sad, confident, fast, slow.
- Save each as `prompts/text_###.txt` and produce a `_manifest.txt`.

Examples:
```
"Hey, quick question‚Äîare you free later this afternoon?"
"Good afternoon. Thank you all for joining us today."
"Everything is okay. Take a slow breath and relax."
```

---

## üß† Step 2 ‚Äì Experiment Config
Use `experiment_config.json` with grids:

```json
{
  "systems": {
    "S0": "",
    "S1": "DEFAULT",
    "S2": "You are an AI assistant designed to convert text into speech... Preserve the speaker's identity; avoid changing style, pitch, or timbre."
  },
  "scenes": {
    "D1": "Audio is recorded from a quiet room.",
    "D2": "Audio is recorded via a mobile phone call in a noisy cafe.",
    "D3": "Audio is recorded in a small office with mild reverberation."
  },
  "decoding": {
    "K1": {"temp": 0.7, "top_p": 0.9, "top_k": 20},
    "K2": {"temp": 0.5, "top_p": 0.85, "top_k": 10},
    "K3": {"temp": 1.0, "top_p": 0.95, "top_k": 50},
    "K4": {"temp": 1.2, "top_p": 0.98, "top_k": 100}
  },
  "tags": {"T0": "", "T1": "[SPEAKER1]"},
  "references": {
    "R1": ["./ref-audio/ref1.wav"],
    "R3": ["./ref-audio/ref1.wav", "./ref-audio/ref2.wav", "./ref-audio/ref3.wav"]
  }
}
```

---

## ‚öôÔ∏è Step 3 ‚Äì API Call Wrapper
In `scripts/higgs_eval.py`, add:

```python
def synthesize_audio(client, system_text, scene_text, ref_paths, speaker_tag, content_text, decoding):
    # Build system (use DEFAULT if requested)
    system = (
        "You are an AI assistant designed to convert text into speech.\n"
        "If the user's message includes a [SPEAKER*] tag, do not read out the tag and generate speech for the following text, using the specified voice.\n"
        "If no speaker tag is present, select a suitable voice on your own.\n\n"
        "<|scene_desc_start|>\n" + scene_text + "\n<|scene_desc_end|>"
    ) if system_text == "DEFAULT" else system_text

    messages = [{"role": "system", "content": system}]
    # Provide reference transcript and attach reference audio(s)
    messages.append({"role": "user", "content": "[SPEAKER1] Reference sample for speaker identity."})
    for rp in ref_paths:
        messages.append({
            "role": "assistant",
            "content": [{
                "type": "input_audio",
                "input_audio": {"data": b64(rp), "format": "wav"}
            }],
        })
    # Final content
    final_text = f"{speaker_tag} {content_text}".strip()
    messages.append({"role": "user", "content": final_text})

    resp = client.chat.completions.create(
        model="higgs-audio-generation-Hackathon",
        messages=messages,
        modalities=["text","audio"],
        temperature=decoding["temp"],
        top_p=decoding["top_p"],
        max_completion_tokens=4096,
        stop=["<|eot_id|>","<|end_of_text|>","<|audio_eos|>"],
        stream=False,
        extra_body={"top_k": decoding["top_k"]},
    )
    # Return audio bytes + latency (implement extraction as per API response)
    return resp
```

---

## üî¨ Step 4 ‚Äì Evaluation & Logging
- Extract generated audio, compute **ECAPA** cosine similarity vs reference, **WER** via Whisper-Large, and **latency**.
- Append rows to `experiments/higgs_prompt_eval.csv` with columns:

```
block,system,scene,speaker_tag,temp,top_p,top_k,sim_ecapa,wer,latency_s,content,out_wav
```

---

## üß© Step 5 ‚Äì Batch Runner
Create `scripts/run_experiments.py` that:
- Loads `experiment_config.json` and the generated prompts.
- Runs **Baseline Block**: `S1 + D1 + T1 + R1 + K3` across all contents.
- Runs **ablations** (vary one axis at a time): S0/S2, D2/D3, K1/K2/K4, T0, R3.
- Saves each output wav under `outputs/{block_name}/` and appends metrics to the CSV.

---

## ‚úÖ Constraints
- Do **not** change core scoring/training logic. Only add wrappers/utilities.
- Keep the API call structure identical aside from parameter changes.
- Make functions return JSON-serializable dicts for easy logging.

---

## üì¶ Deliverables
1. `tools/generate_prompt_texts.py`  
2. `experiment_config.json`  
3. `scripts/higgs_eval.py` (with `synthesize_audio` wrapper)  
4. `scripts/run_experiments.py`  
5. `experiments/higgs_prompt_eval.csv`

---
